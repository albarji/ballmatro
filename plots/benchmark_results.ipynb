{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a90ce2",
   "metadata": {},
   "source": [
    "# Notebook to generate plots from the benchmarks results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91af309",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837540f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_COLORS = {\n",
    "    \"gpt-4o-mini-2024-07-18\": \"#8c8c8c\",\n",
    "    \"gpt-4o-2024-08-06\": \"#000000\",\n",
    "    \"gpt-4.1-nano-2025-04-14\": \"#DCB6B6\",\n",
    "    \"gpt-4.1-mini-2025-04-14\": \"#935151\",\n",
    "    \"gpt-4.1-2025-04-14\": \"#933535\",\n",
    "    \"o4-mini-2025-04-16\": \"#929530\",\n",
    "}\n",
    "\n",
    "MODEL_ORDER = [\n",
    "    \"gpt-4.1-nano-2025-04-14\",\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    \"gpt-4.1-mini-2025-04-14\",\n",
    "    \"gpt-4o-2024-08-06\",\n",
    "    \"gpt-4.1-2025-04-14\",\n",
    "    \"o4-mini-2025-04-16\"\n",
    "]\n",
    "\n",
    "MODEL_SHORT_NAMES = {\n",
    "    \"gpt-4o-mini-2024-07-18\": \"4o-mini\",\n",
    "    \"gpt-4o-2024-08-06\": \"4o\",\n",
    "    \"gpt-4.1-nano-2025-04-14\": \"4.1-nano\",\n",
    "    \"gpt-4.1-mini-2025-04-14\": \"4.1-mini\",\n",
    "    \"gpt-4.1-2025-04-14\": \"4.1\",\n",
    "    \"o4-mini-2025-04-16\": \"o4-mini\"\n",
    "}\n",
    "\n",
    "LEVEL_EXPLANATIONS = {\n",
    "    \"level1\": \"1 card\",\n",
    "    \"level2\": \"2 cards\",\n",
    "    \"level3\": \"4 cards\",\n",
    "    \"level4\": \"8 cards\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46413276",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462924b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BENCHMARKS_FOLDER = \"../benchmarks\"\n",
    "\n",
    "def load_raw_benchmark_data():\n",
    "    \"\"\"Loads benchmark data as a dictionary from level and model name to results.\"\"\"\n",
    "    data = {}\n",
    "    for file in glob.glob(f\"{BENCHMARKS_FOLDER}/*.json\"):\n",
    "        # Get name of file without path and extension\n",
    "        name = Path(file).stem\n",
    "        level, model = name.split(\"_\")\n",
    "        with open(file, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "            data[(level, model)] = json_data\n",
    "    return data\n",
    "\n",
    "def raw_data_to_dataframe(data: dict):\n",
    "    \"\"\"Converts raw benchmark data to a pandas DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    for (level, model), results in data.items():\n",
    "        row = {\n",
    "            \"level\": level,\n",
    "            \"level_with_explanation\": f\"{level} ({LEVEL_EXPLANATIONS.get(level, 'Unknown')})\",\n",
    "            \"model\": model,\n",
    "            \"model_shortname\": MODEL_SHORT_NAMES.get(model, model),\n",
    "            \"model_color\": MODEL_COLORS.get(model, \"#000000\"),\n",
    "            \"total_score\": results[\"total_score\"],\n",
    "            \"total_normalized_score\": results[\"total_normalized_score\"],\n",
    "            \"invalid_hands\": results[\"invalid_hands\"],\n",
    "            \"normalized_invalid_hands\": results[\"normalized_invalid_hands\"],\n",
    "        }\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.model = df.model.astype(\"category\")\n",
    "    df.model = df.model.cat.set_categories(MODEL_ORDER)\n",
    "    df.sort_values(by=[\"level\", \"model\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc159f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_raw_benchmark_data()\n",
    "df = raw_data_to_dataframe(raw_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cb37f",
   "metadata": {},
   "source": [
    "## Plot performances by level an model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca30ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[df[\"level_with_explanation\"], df[\"model_shortname\"]],\n",
    "        y=df[\"total_normalized_score\"],\n",
    "        text=[f'{x:.0%}' for x in df[\"total_normalized_score\"]],\n",
    "        marker={\n",
    "            \"color\": df[\"model_color\"],\n",
    "        }, \n",
    "    )\n",
    ")\n",
    "fig.update_yaxes(tickformat=\",.0%\", range=[0, 1], title=\"Normalized score\")\n",
    "fig.update_xaxes(title=\"Difficulty level and model\",)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed6efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ballmatro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
